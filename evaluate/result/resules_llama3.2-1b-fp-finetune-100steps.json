{
  "model_path": "../model_quantization/llama3.2-1b-fp-finetune-100steps",
  "quant_mode": "none",
  "data_dir": "../datasets/mergedata_preprocessed",
  "model_size_mb_via_save": 2357.227849006653,
  "evaluation_date": "2025-11-22 10:00:31",
  "test": {
    "ppl": 6.599816847785709,
    "avg_loss": 1.8870418983118336,
    "loss_std": 0.21485944092273712,
    "loss_min": 1.1920197010040283,
    "loss_max": 2.650195598602295,
    "total_tokens": 1912813,
    "samples_evaluated": 7757
  },
  "throughput": {
    "mean_latency_ms": 348.3022165298462,
    "tokens_per_sec": 23519.804386022402,
    "batch_size": 8,
    "seq_len": 1024,
    "num_runs": 50
  },
  "throughput_compiled": {
    "mean_latency_ms": 316.53783321380615,
    "tokens_per_sec": 25880.002768789716,
    "batch_size": 8,
    "seq_len": 1024,
    "num_runs": 50
  }
}