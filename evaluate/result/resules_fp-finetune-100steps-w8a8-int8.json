{
  "model_path": "../model_quantization/llama3.2-1b-fp-finetune-100steps-w8a8-int8",
  "quant_mode": "offline_w8a8_baseline_100epochs",
  "data_dir": "../datasets/mergedata_preprocessed",
  "model_size_mb_via_save": 1430.0148820877075,
  "evaluation_date": "2025-11-22 09:04:02",
  "test": {
    "ppl": 6.643278405727709,
    "avg_loss": 1.8936055774432923,
    "loss_std": 0.21586744487285614,
    "loss_min": 1.195522427558899,
    "loss_max": 2.658177614212036,
    "total_tokens": 1912813,
    "samples_evaluated": 7757
  },
  "throughput": {
    "mean_latency_ms": 570.1430988311768,
    "tokens_per_sec": 14368.322648812253,
    "batch_size": 8,
    "seq_len": 1024,
    "num_runs": 50
  },
  "throughput_compiled": {
    "mean_latency_ms": 198.55084419250488,
    "tokens_per_sec": 41258.95325862956,
    "batch_size": 8,
    "seq_len": 1024,
    "num_runs": 50
  }
}